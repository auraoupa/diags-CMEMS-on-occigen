/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:45889'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:35105'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:44565'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:43628'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:33693'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:33649'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:35661'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:36527'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:40169'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:35739'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:39286'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:46610'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:43129'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:38306'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:38793'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:44709'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:34572'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:37549'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:41814'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:34945'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:32984'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:45646'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:36116'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:44541'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:40854'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:36659'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:44702'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.136:35302'
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ga_k8fn1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-h25vfnad', purging
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:36483
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:42140
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:36367
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:43388
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:36483
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:43388
distributed.worker - INFO -              nanny at:        172.30.10.136:34572
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:42140
distributed.worker - INFO -              bokeh at:        172.30.10.136:36589
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:36367
distributed.worker - INFO -              nanny at:        172.30.10.136:44565
distributed.worker - INFO -              nanny at:        172.30.10.136:33693
distributed.worker - INFO -              nanny at:        172.30.10.136:40169
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -              bokeh at:        172.30.10.136:35186
distributed.worker - INFO -              bokeh at:        172.30.10.136:33844
distributed.worker - INFO -              bokeh at:        172.30.10.136:37498
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-w7ph2e_y
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-hf0sl2w0
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-7iza4adj
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-mvkv0qln
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:40557
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:40557
distributed.worker - INFO -              nanny at:        172.30.10.136:40854
distributed.worker - INFO -              bokeh at:        172.30.10.136:43422
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:34629
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:34629
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -              nanny at:        172.30.10.136:36527
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ryoz0vwh
distributed.worker - INFO -              bokeh at:        172.30.10.136:37683
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-7mx495nh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:41631
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:41631
distributed.worker - INFO -              nanny at:        172.30.10.136:38306
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:43229
distributed.worker - INFO -              bokeh at:        172.30.10.136:39951
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:43229
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              nanny at:        172.30.10.136:32984
distributed.worker - INFO -              bokeh at:        172.30.10.136:45106
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-imhndtgk
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-cksulmuq
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:42461
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:42461
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:46853
distributed.worker - INFO -              nanny at:        172.30.10.136:35105
distributed.worker - INFO -              bokeh at:        172.30.10.136:43154
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:46853
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -              nanny at:        172.30.10.136:35661
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -              bokeh at:        172.30.10.136:38658
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_mdluyex
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-bceqtsa1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-nkgkwy3u', purging
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:42670
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:42670
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:34951
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:34951
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -              nanny at:        172.30.10.136:45889
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              nanny at:        172.30.10.136:44541
distributed.worker - INFO -              bokeh at:        172.30.10.136:34445
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:        172.30.10.136:38520
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-jxlsvxws
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-hzhrl80b
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:36489
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:36489
distributed.worker - INFO -              nanny at:        172.30.10.136:34945
distributed.worker - INFO -              bokeh at:        172.30.10.136:39651
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:45292
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:45292
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-xfg1ujjg
distributed.core - INFO - Starting established connection
distributed.worker - INFO -              nanny at:        172.30.10.136:45646
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:        172.30.10.136:43391
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-f3dqrt69
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:40739
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:40739
distributed.worker - INFO -              nanny at:        172.30.10.136:37549
distributed.worker - INFO -              bokeh at:        172.30.10.136:34441
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-clgvjmdj
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:34288
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:34288
distributed.worker - INFO -              nanny at:        172.30.10.136:44702
distributed.worker - INFO -              bokeh at:        172.30.10.136:45285
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:39334
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:39334
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ik6bghcb
distributed.worker - INFO -              nanny at:        172.30.10.136:36116
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:        172.30.10.136:37488
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:42557
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:42557
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -              nanny at:        172.30.10.136:36659
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-2x22_cuh
distributed.worker - INFO -              bokeh at:        172.30.10.136:37294
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:43526
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-qteqqpeg
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:43526
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              nanny at:        172.30.10.136:35739
distributed.worker - INFO -              bokeh at:        172.30.10.136:34556
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-kq3ccjw5', purging
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-c6b2op83
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:36525
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:36525
distributed.worker - INFO -              nanny at:        172.30.10.136:39286
distributed.worker - INFO -              bokeh at:        172.30.10.136:43514
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-lmnr_7nv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:35550
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:35550
distributed.worker - INFO -              nanny at:        172.30.10.136:35302
distributed.worker - INFO -              bokeh at:        172.30.10.136:45959
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:43819
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:42517
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:43819
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:44492
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:42517
distributed.worker - INFO -              nanny at:        172.30.10.136:43628
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:44492
distributed.worker - INFO -              nanny at:        172.30.10.136:44709
distributed.worker - INFO -              bokeh at:        172.30.10.136:42756
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -              nanny at:        172.30.10.136:41814
distributed.worker - INFO -              bokeh at:        172.30.10.136:39134
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -              bokeh at:        172.30.10.136:33149
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-48cvops8
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-vm2b9rp5
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-vlib5krn
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-i9q9f7pf
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:45950
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:41448
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:45950
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:41448
distributed.worker - INFO -              nanny at:        172.30.10.136:33649
distributed.worker - INFO -              nanny at:        172.30.10.136:43129
distributed.worker - INFO -              bokeh at:        172.30.10.136:39072
distributed.worker - INFO -              bokeh at:        172.30.10.136:42682
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-2l74tn2w
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-4fnnj25j
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-wxo8fpbi', purging
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:33336
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:33336
distributed.worker - INFO -              nanny at:        172.30.10.136:38793
distributed.worker - INFO -              bokeh at:        172.30.10.136:33661
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1tocoblb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1id3ciju', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-jpvavyj6', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-x94_w5lj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-trs0i3kl', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6x02nbpo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-zz78_bq8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-p150mk59', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-e7cozk6w', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-jm_3c38b', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-dqsug0zg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-f4bajr1s', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-d6tzu2mz', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-yak46pf3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pyufaa21', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-8t1ww07l', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-4jdosltu', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-xy4bwesh', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pdt3yvzo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-imr3mdh4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-lavqjzza', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-wi3ltzto', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6elo6dnm', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-p473ic6d', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-bq07kmwz', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-cm4fsaps', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-avam1brs', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6fmtbi32', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-i36ruih6', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-kr8bar6g', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-kkzl1iyf', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-g9agy7sq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pcfx7akw', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-cmjodnxp', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9fbg21bq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-7jmvmw0f', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-7o303mrm', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6oijmwa9', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-jxvrakr2', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-gcjy20yu', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-8jkt48j4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-u8zsaexw', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-af8d4b2f', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-wjep5msl', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-k7rcx_k3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-k5w4ksfr', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-rj1pyqb_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ysxeaebu', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-8ga8zd0r', purging
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0zhxdaqn', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-kndg84go', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-wyito1w4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-gqqagjax', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-z4otq1d9', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-yu7wsahp', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-oeloaqv2', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-tj40rmx7', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-g3o51_j2', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-fho5fe2e', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1tbbixv1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-jeypzhei', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-t9pocy88', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_ug531p5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-sylk49x7', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-vcunk4db', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-nzyvu4uw', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-d3e4fb9s', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-7j40lwmh', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-7fqo37rf', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-cmc38luf', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-nalgy3ss', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-trc44hv1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-h46qbot_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pbbxdosd', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-qhiqwopg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1glq30kv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-getdm281', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-il6gu480', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-blokrqam', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-kf9kbbov', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-sn6k7c9s', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-8_3tpvwd', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-df58cam4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-za8l1rlh', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-xugv07l7', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-u64qhjsj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-801i0zbn', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-gf0zryxe', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-kxdqiz7p', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-gjj8oxe9', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-tkf9v2vr', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pxbr31oo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-r04ff_wq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-er_1lu7x', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-zmv89h7d', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-8uq58syt', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-eim1ojfj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-v6yv9ce5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-qq55jpas', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-sy4ellqg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_vi694by', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-zochaxm3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-nq5mjffk', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-lmj42w6e', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-uzq4zea9', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-4lsfybd3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0iyg6rqx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-xvlap6dr', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-8q1cys2u', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1hc89zbh', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6vipr_cg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-2bhylwhm', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-moyod6l1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-nx2j_t6b', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-yr75j47r', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6wysydm_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-lmev7zc0', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-uz651bpt', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-tltlrvmc', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ny1t9v0s', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-s33ukkfx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-i20lz0jq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-xgvfh002', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-v4_54my_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-f9ldhzak', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-8ezge4hf', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-jw1oiwww', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pd12y1ei', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-mijjz6tq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-b6tkzuqe', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-nv4_qm40', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-f5ch5rmk', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-2z4ak4km', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ae31dqfv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-29_kzcbt', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-00mxc2_5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-li5xkssg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-d6_bzqgw', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ky0mscrn', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-75jf0qrs', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-elx9_fwm', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-t7qoxeez', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-x0pqcrk2', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-lnz30xx4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-bygpwxoi', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-vk9l_uco', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-afbfngi3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ik_356l3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-oocas4tg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0czj_9b8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-dq0_ht5g', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ioicybpq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-x46kh5m7', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-s1lnkbk8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-i6pjdvup', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-w5h1f_uo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0ve98f2u', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-2n16t8i_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-k5s3voj7', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-l937j001', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-l6cozhnq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-mhqj560x', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-lhz0toqj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-5o_bsstk', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-uofzcvmo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-y4o78l35', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-h7fztopy', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-myjultg3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1xf0kobv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-o6an1j62', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_9qve8ic', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-52u9d8mv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-dvduzt0n', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-kbr5bfcg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ek645rz6', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-57guh002', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-gbdd8g3n', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pz0k9t3e', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-di57yuso', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-hu9ydat9', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-g76fju3c', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pwerbk09', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_bi5g949', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-2wv6ul5p', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-t7ny5532', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-rcrpy0g6', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_v6i25hw', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-zvj0ypmf', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-tql97iif', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-e545h6l2', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-01qs9yjb', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-8ohkwvry', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9qj5u806', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ixhoccno', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-yxmn4gg5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-5yfaserl', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-qk9mfxih', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-h0_w94od', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-k0_bck70', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-o2kf9xth', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-09ojs1t5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-4l3qddeb', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pusx4k3i', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_b29u9z5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-yfi3eb0a', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-61o_px2n', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-il3742ji', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-8riatkpe', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-wqdb4w8m', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0xnfvs03', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-h4dfodey', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-yy327ung', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-wgrvuzfe', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-4856ybqp', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-o0qqasjv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-j2tz359w', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-we3a0x98', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-v40nk7em', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-nlhyaywq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-3upo0zcx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-n0239sh3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-uox0895s', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-x38axz9v', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-43cmdl88', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-x_l39h5u', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-p45rlgdb', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-peil6t4n', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-3scd2rmk', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-xeoj58d5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-r0guxnrk', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pvw5rjn9', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-tsxqjb6b', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_jfgbm4g', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-y_za2_gc', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1lqzynkz', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-nwpcusm4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-vopiv6o8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-fx5jv8ga', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-f5is86b6', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-95u89luo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-3ffw6qn8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-opb4s36_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-rhk895a8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-phd8fmbc', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-l8scmzx8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-kq387fts', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-47kjwxoq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1sjtf3k8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ewr6wl35', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-j0n_v0k2', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-99i0rooa', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-av_zygmo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0_vfc0us', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-va2kknfb', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0xvh5h2n', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-mv3g0ajb', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-zln4cgx3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1krrika1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-un1gex3e', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-aihgzlij', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-e9h333ik', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_zo_7ftk', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_pal_6uu', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-e77p3663', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-roziveh0', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1ny3bopk', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_m49ptql', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-wyh9r4xt', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-j946xu_l', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-hd6d_l2w', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-kqtek563', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-gz1__ubx', purging
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:  tcp://172.30.10.136:43521
distributed.worker - INFO -          Listening to:  tcp://172.30.10.136:43521
distributed.worker - INFO -              nanny at:        172.30.10.136:46610
distributed.worker - INFO -              bokeh at:        172.30.10.136:43156
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-wjti2xwy
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.utils_perf - INFO - full garbage collection released 13.95 MB from 772 reference cycles (threshold: 10.00 MB)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38521 remote=tcp://172.30.100.1:42394>
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ad964624208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ad964624208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ad964624208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ad964624208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ad964624208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ad964624208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ad964624208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38848 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39170 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39264 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38850 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39156 remote=tcp://172.30.100.1:42394>
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ad964624208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ad964624208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ad964624208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39484 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39762 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39656 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40010 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39988 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40092 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40088 remote=tcp://172.30.100.1:42394>
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ad964624208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ad964624208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38944 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39760 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40106 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39098 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39814 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39152 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40184 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39294 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39026 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40078 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39073 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39124 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39842 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39184 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39236 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39888 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39268 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39918 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39322 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39962 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39436 remote=tcp://172.30.100.1:42394>
distributed.utils_perf - INFO - full garbage collection released 31.59 MB from 1540 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38946 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39214 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39990 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39002 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40108 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39100 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39816 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39154 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40186 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39256 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39902 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39296 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40050 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39860 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39900 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38998 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39812 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39150 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40144 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39210 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40182 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39292 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39934 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39348 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39986 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40048 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39800 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39135 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39198 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40164 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39884 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39248 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39894 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39278 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39930 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39335 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39974 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39366 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40032 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39450 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39260 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40056 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40114 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39106 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39820 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39220 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40212 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39302 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39390 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39490 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39546 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40192 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39600 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39714 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39766 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39944 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38602 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40024 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39028 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40082 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39128 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39844 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39188 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39240 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39922 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39324 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39966 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39440 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39514 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39568 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39624 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39680 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39738 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38908 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40012 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39796 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39836 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39172 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39872 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39228 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39912 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39316 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39956 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39404 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40070 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39426 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39472 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39502 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39560 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39616 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39672 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39726 remote=tcp://172.30.100.1:42394>
distributed.utils_perf - INFO - full garbage collection released 39.89 MB from 2356 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40390 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40090 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39138 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39196 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40166 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39879 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39242 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39892 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39280 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39926 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39330 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39970 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39368 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40030 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39444 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39480 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40130 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39520 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39574 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39634 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39690 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39744 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39850 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39658 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40058 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39058 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39104 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39822 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39862 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39218 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39392 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39488 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39544 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40190 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39602 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39712 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39768 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39942 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39996 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40378 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40384 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40388 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40640 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40638 remote=tcp://172.30.100.1:42394>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40376 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39590 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39696 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38938 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39806 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40140 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39202 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39342 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39376 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40046 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40100 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39458 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39533 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39642 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39752 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39030 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40080 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39074 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39126 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39846 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39186 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39238 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39890 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39270 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39920 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39326 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39964 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39410 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39438 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39512 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39570 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39626 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39682 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39736 remote=tcp://172.30.100.1:42394>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40062 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40116 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39108 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39824 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39164 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39224 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39262 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39306 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40000 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39394 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39470 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39494 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39550 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39604 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39662 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39716 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39772 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39946 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39702 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38936 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39804 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40138 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39208 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39252 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39346 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39380 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40044 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40098 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39460 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39528 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39640 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39754 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40664 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40730 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39176 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39674 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38912 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40014 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39778 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39022 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40074 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39068 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39120 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39838 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39232 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39914 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39318 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39958 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39416 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39424 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39506 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39562 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39618 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39730 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39780 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39146 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39586 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39932 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39698 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38934 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39810 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40142 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39206 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39254 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39344 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39378 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40042 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40102 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39454 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39530 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39644 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39756 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40686 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38892 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39992 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40054 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39818 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39216 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40210 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39904 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39298 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39486 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39542 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39598 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39654 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39710 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39764 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39940 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40110 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38538 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39500 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39668 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39774 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39018 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39116 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39832 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39870 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39226 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40202 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39908 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39312 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40006 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39402 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40068 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39430 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39556 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39612 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39724 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39952 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40052 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39538 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39594 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39650 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39936 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40694 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40750 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40806 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40428 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40912 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40472 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40526 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40582 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41284 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41238 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41282 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41236 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39584 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39700 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38940 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39808 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39204 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39340 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40040 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39414 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40096 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39456 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39532 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39646 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39750 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40688 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40800 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40855 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40905 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40518 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40576 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40632 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39250 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39140 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39858 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39200 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40172 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40240 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39282 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39338 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39978 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39374 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40038 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40094 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39452 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39526 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39582 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39638 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39694 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39748 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39790 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40794 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40392 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40902 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40514 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40996 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40570 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40626 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40682 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39400 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39016 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39114 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39830 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39868 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39310 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40066 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39428 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39498 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39554 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39610 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39722 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40878 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40924 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41022 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40597 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40820 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39776 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39020 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40072 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39066 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39118 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40122 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39834 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39174 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39230 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39886 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39266 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39910 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39314 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39406 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39504 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39558 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39614 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39670 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39728 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40770 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40826 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40884 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40448 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40928 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40492 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40546 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40602 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40716 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40408 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38610 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40086 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39798 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39132 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39194 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40168 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39878 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39244 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39898 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39274 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39924 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39332 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39972 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39372 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40036 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39447 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40136 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39522 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39636 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39692 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39742 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39854 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40734 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40898 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40456 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40506 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40988 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40564 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40622 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40676 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40786 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40842 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40366 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41324 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41418 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41198 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38734 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38846 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39786 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38790 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38904 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39014 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39794 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39112 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39828 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39866 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39308 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40064 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39432 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39496 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39552 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39608 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39720 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40876 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40922 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40486 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41024 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40596 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40822 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41346 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41392 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39802 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39136 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39192 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40170 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39882 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39246 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39896 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39276 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39928 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39336 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39976 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39370 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40034 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39446 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40134 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39523 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39632 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39688 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39746 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39856 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40732 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40900 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40462 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40510 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40994 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40566 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40624 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40678 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40790 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40846 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40370 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41286 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41326 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41366 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41416 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41200 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41468 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38606 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:38810 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40028 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40084 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40120 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39130 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39848 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39190 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39272 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39328 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39364 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39412 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39442 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39478 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39516 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39572 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39628 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39684 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40840 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40986 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40560 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40616 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40672 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40728 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40784 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40896 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40940 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41362 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41410 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41522 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40060 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40118 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39110 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39826 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39162 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39864 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39222 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39906 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39304 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39998 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39396 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39468 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39492 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39548 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39606 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39660 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39718 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39770 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:39948 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40760 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40816 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40872 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40438 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40482 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40538 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40594 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:40648 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41344 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41388 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41498 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41218 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.136:41306 remote=tcp://172.30.100.1:42394>
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:43526
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:43229
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:42140
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:42461
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:42557
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:45950
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:42670
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:44492
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:34629
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:39334
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:43819
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:41631
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:42517
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:34951
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:36489
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:45292
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:46853
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:34288
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:35550
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:43521
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:36367
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:36483
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:41448
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:36525
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:40557
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:40739
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:33336
distributed.worker - INFO - Stopping worker at tcp://172.30.10.136:43388
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:35739'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:44702'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:32984'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:36659'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:45889'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:40854'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:35105'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:40169'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:44565'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:33693'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:33649'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:44709'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:34572'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:35302'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:36527'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:41814'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:35661'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:34945'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:44541'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:43129'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:36116'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:38306'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:39286'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:45646'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:43628'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:38793'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:46610'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.136:37549'
distributed.dask_worker - INFO - End worker
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
