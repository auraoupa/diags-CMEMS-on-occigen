/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:42882'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:45279'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:37298'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:33950'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:44825'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:34311'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:46601'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:41276'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:45609'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:42523'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:42887'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:32824'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:42585'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:34325'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:45099'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:37164'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:33794'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:45763'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:35674'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:38439'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:36264'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:35792'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:46146'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:35018'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:45245'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:34359'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:37426'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.23:40090'
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:33878
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:46290
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:39102
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:33878
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:34808
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:46290
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:39102
distributed.worker - INFO -              nanny at:          172.30.9.23:33794
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:34808
distributed.worker - INFO -              bokeh at:          172.30.9.23:43504
distributed.worker - INFO -              nanny at:          172.30.9.23:45609
distributed.worker - INFO -              nanny at:          172.30.9.23:45099
distributed.worker - INFO -              nanny at:          172.30.9.23:32824
distributed.worker - INFO -              bokeh at:          172.30.9.23:42355
distributed.worker - INFO -              bokeh at:          172.30.9.23:41214
distributed.worker - INFO -              bokeh at:          172.30.9.23:41332
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:33867
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:33867
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -              nanny at:          172.30.9.23:37426
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -              bokeh at:          172.30.9.23:37152
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-00kcjeye
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ju_qgsj0
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-gv2y2wf9
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-8c7ljbty
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_qp1nmka
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:45045
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:45045
distributed.worker - INFO -              nanny at:          172.30.9.23:40090
distributed.worker - INFO -              bokeh at:          172.30.9.23:41808
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-abd3whd6
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:35558
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:35558
distributed.worker - INFO -              nanny at:          172.30.9.23:36264
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:35287
distributed.worker - INFO -              bokeh at:          172.30.9.23:40253
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:35287
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -              nanny at:          172.30.9.23:37298
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -              bokeh at:          172.30.9.23:42841
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9jrzikki
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-zxywt148
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:34692
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:34692
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO -              nanny at:          172.30.9.23:42523
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:          172.30.9.23:37036
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:43132
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:43132
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-gdpqkrn1
distributed.worker - INFO -              nanny at:          172.30.9.23:45279
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:          172.30.9.23:33180
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-zns_dkuu
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:35994
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:35994
distributed.worker - INFO -              nanny at:          172.30.9.23:35674
distributed.worker - INFO -              bokeh at:          172.30.9.23:34824
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-lpwvmvux
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:37142
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:37142
distributed.worker - INFO -              nanny at:          172.30.9.23:44825
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:45261
distributed.worker - INFO -              bokeh at:          172.30.9.23:35867
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:45261
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -              nanny at:          172.30.9.23:46601
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:          172.30.9.23:45262
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-3hvz4ke1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-lp11rrtt
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:45795
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:45795
distributed.worker - INFO -              nanny at:          172.30.9.23:45245
distributed.worker - INFO -              bokeh at:          172.30.9.23:43202
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-sq5ggwws
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:41765
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:41765
distributed.worker - INFO -              nanny at:          172.30.9.23:37164
distributed.worker - INFO -              bokeh at:          172.30.9.23:40186
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-zf90arv_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:33052
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:33052
distributed.worker - INFO -              nanny at:          172.30.9.23:35792
distributed.worker - INFO -              bokeh at:          172.30.9.23:38595
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-zdn21bjy
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:38347
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:38347
distributed.worker - INFO -              nanny at:          172.30.9.23:42882
distributed.worker - INFO -              bokeh at:          172.30.9.23:37411
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:33173
distributed.worker - INFO -                Memory:                    4.29 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:37024
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-nkkf_3jx
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:33173
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:37024
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              nanny at:          172.30.9.23:33950
distributed.worker - INFO -              bokeh at:          172.30.9.23:45818
distributed.worker - INFO -              nanny at:          172.30.9.23:45763
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -              bokeh at:          172.30.9.23:38052
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:37058
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:37058
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pin1hhk3
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ocp5iht3
distributed.worker - INFO -              nanny at:          172.30.9.23:42887
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:          172.30.9.23:35295
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-fs1wndpo
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:38787
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:38787
distributed.worker - INFO -              nanny at:          172.30.9.23:34325
distributed.worker - INFO -              bokeh at:          172.30.9.23:33303
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-t66ezql2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:33637
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:33637
distributed.worker - INFO -              nanny at:          172.30.9.23:34359
distributed.worker - INFO -              bokeh at:          172.30.9.23:33044
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-d7jhxzj1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:42382
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:42382
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:39747
distributed.worker - INFO -              nanny at:          172.30.9.23:35018
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:39747
distributed.worker - INFO -              bokeh at:          172.30.9.23:38326
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -              nanny at:          172.30.9.23:42585
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:          172.30.9.23:36245
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pw0dybco
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-x27kh_y1
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:45381
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:45381
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:39699
distributed.worker - INFO -              nanny at:          172.30.9.23:38439
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:39699
distributed.worker - INFO -              bokeh at:          172.30.9.23:45488
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO -              nanny at:          172.30.9.23:46146
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:          172.30.9.23:38047
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-lgiev8xm
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-fz7aj2kn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:38282
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:38282
distributed.worker - INFO -              nanny at:          172.30.9.23:34311
distributed.worker - INFO -              bokeh at:          172.30.9.23:36040
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-70luprbz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.23:42347
distributed.worker - INFO -          Listening to:    tcp://172.30.9.23:42347
distributed.worker - INFO -              nanny at:          172.30.9.23:41276
distributed.worker - INFO -              bokeh at:          172.30.9.23:42367
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-8vuubzku
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:40366 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:40370 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:40368 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:40372 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:40374 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:40376 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:40378 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56660 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56662 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56664 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56672 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56696 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56698 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56692 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56668 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56658 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56686 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56666 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56652 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56674 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56654 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56656 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56648 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56650 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56676 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56678 remote=tcp://172.30.8.67:37053>
distributed.utils_perf - INFO - full garbage collection released 11.17 MB from 693 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54018 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54028 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54040 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54154 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54176 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53728 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54178 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54180 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53730 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53732 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54022 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54024 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54172 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53724 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54174 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53726 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54038 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53704 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54034 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54032 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54200 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53750 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54030 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54202 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53754 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53998 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53718 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54226 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54026 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54194 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54224 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54020 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53798 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54014 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54036 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54204 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54264 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53748 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54252 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53802 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54272 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54222 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54336 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53888 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54208 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54262 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53812 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54318 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53870 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53926 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53984 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54094 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54152 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53816 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54320 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53872 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53928 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53980 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54092 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54148 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54302 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54352 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53906 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54414 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53966 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54074 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54128 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55360 remote=tcp://172.30.100.1:40975>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55364 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54908 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53796 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54300 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54358 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53910 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54408 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53964 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54078 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54134 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54924 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53792 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54298 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53908 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54410 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53962 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54072 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54130 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54886 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55726 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55342 remote=tcp://172.30.100.1:40975>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54906 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54332 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53884 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53940 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54388 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53996 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54052 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54108 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54164 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54220 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54276 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54928 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55876 remote=tcp://172.30.100.1:40975>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54930 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55874 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55868 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56138 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55878 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54230 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53780 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54286 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53838 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54344 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53894 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53950 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54008 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54060 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54116 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54254 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53806 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54310 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53864 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53918 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53974 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54086 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54142 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54334 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53886 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54390 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53942 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54446 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54054 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54110 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54166 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54278 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55498 remote=tcp://172.30.100.1:40975>
distributed.utils_perf - INFO - full garbage collection released 19.73 MB from 1321 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56142 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54256 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53808 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54312 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53866 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53920 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53978 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54090 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54146 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56166 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54270 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53822 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54382 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53934 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53990 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54046 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54102 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54158 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53670 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56144 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56290 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56420 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54198 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53752 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54258 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53810 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54314 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53862 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53922 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53976 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54088 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54144 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55822 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55934 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56400 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55986 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55100 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56454 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56046 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55154 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56514 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56098 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55208 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55264 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56212 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55320 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55378 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56300 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55432 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56352 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55488 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54234 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53786 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54290 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53842 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54346 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53898 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53952 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54010 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54066 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54122 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55854 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54964 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56330 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55910 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55132 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55188 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56134 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55244 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55300 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56244 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55356 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56288 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55412 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56378 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55524 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56434 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55580 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56490 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55636 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55692 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54206 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54260 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53814 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54316 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53868 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53924 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53982 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54096 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54150 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56272 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55824 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54992 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56356 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55938 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55046 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56406 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55992 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55102 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56050 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55160 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56516 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55214 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56162 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55272 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56216 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55328 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55384 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56306 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55438 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55552 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55610 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55664 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55718 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55772 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55884 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54228 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53784 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54288 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53840 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54342 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53896 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53954 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54006 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54062 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54120 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54904 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55852 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54962 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56324 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55906 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55130 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55186 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56132 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55242 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55298 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56242 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55354 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56286 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55406 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56376 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55518 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56428 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55578 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56488 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55634 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55688 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55742 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55798 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55960 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56020 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56076 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56188 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53722 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54392 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53946 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54448 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54002 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54498 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54056 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54114 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54170 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54280 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54844 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54900 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56370 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55956 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55068 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56422 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56014 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55122 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56068 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55178 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55236 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55402 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55514 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55570 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55628 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55684 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55732 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55788 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55846 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55902 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56180 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56236 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56322 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55496 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56276 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55826 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54938 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54994 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55936 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55048 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56408 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55994 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55106 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56048 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55158 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56518 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55216 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56164 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55274 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56218 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55330 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55382 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56308 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55440 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56354 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55550 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55608 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55666 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55722 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55880 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56232 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54894 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55840 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55952 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55062 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55118 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55174 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55230 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55398 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55454 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55510 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55566 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55622 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55678 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55728 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55784 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55896 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55820 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55932 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56404 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55988 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55098 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56044 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55156 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56512 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56102 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55210 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55268 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56210 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55324 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55376 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56302 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55434 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56348 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55492 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55548 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55604 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55660 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55766 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:57520 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53790 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54294 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54350 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53904 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54406 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:53960 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54070 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:54132 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56384 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55970 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55080 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56438 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56030 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56084 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55194 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55248 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56196 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55306 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56252 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55478 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55534 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55586 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55640 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:56550 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55696 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55746 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55802 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55862 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:55914 remote=tcp://172.30.100.1:40975>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:57900 remote=tcp://172.30.100.1:40975>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.23:57898 remote=tcp://172.30.100.1:40975>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67db37cf8>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67db37cf8>: ConnectionRefusedError: [Errno 111] Connection refused
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67db94a58>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67db94a58>: ConnectionRefusedError: [Errno 111] Connection refused
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba7021786a0>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba7021786a0>: ConnectionRefusedError: [Errno 111] Connection refused
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67da6cc88>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67da6cc88>: ConnectionRefusedError: [Errno 111] Connection refused
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67dddab70>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67dddab70>: ConnectionRefusedError: [Errno 111] Connection refused
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67dc29e48>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67dc29e48>: ConnectionRefusedError: [Errno 111] Connection refused
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67f60ccf8>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67f60ccf8>: ConnectionRefusedError: [Errno 111] Connection refused
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67e7247f0>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67e7247f0>: ConnectionRefusedError: [Errno 111] Connection refused
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67d98ce80>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67d98ce80>: ConnectionRefusedError: [Errno 111] Connection refused
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67e025c18>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67e025c18>: ConnectionRefusedError: [Errno 111] Connection refused
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67dfb2240>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67dfb2240>: ConnectionRefusedError: [Errno 111] Connection refused
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67daa7b00>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67daa7b00>: ConnectionRefusedError: [Errno 111] Connection refused
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67d9c8518>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67d9c8518>: ConnectionRefusedError: [Errno 111] Connection refused
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67db363c8>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67db363c8>: ConnectionRefusedError: [Errno 111] Connection refused
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67db7fb70>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67db7fb70>: ConnectionRefusedError: [Errno 111] Connection refused
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67dd9ad68>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67dd9ad68>: ConnectionRefusedError: [Errno 111] Connection refused
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba7024fba20>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba7024fba20>: ConnectionRefusedError: [Errno 111] Connection refused
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67db0a550>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67db0a550>: ConnectionRefusedError: [Errno 111] Connection refused
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67d9554e0>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67d9554e0>: ConnectionRefusedError: [Errno 111] Connection refused
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67dc807f0>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67dc807f0>: ConnectionRefusedError: [Errno 111] Connection refused
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67db55160>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67db55160>: ConnectionRefusedError: [Errno 111] Connection refused
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67df0e748>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67df0e748>: ConnectionRefusedError: [Errno 111] Connection refused
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ba6721d1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67dad04e0>: ConnectionRefusedError: [Errno 111] Connection refused")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:40975' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ba67dad04e0>: ConnectionRefusedError: [Errno 111] Connection refused
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:33173
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:39699
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:37024
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:37058
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:34808
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:45381
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:33052
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:45261
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:39747
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:33878
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:38282
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:33637
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:45045
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:42347
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:34692
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:42382
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:39102
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:35994
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:35558
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:33867
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:38787
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:35287
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:37142
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:41765
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:38347
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:43132
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:45795
distributed.worker - INFO - Stopping worker at tcp://172.30.9.23:46290
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:46146'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:45763'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:32824'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:46601'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:42887'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:33950'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:42882'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:44825'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:42585'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:45099'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:40090'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:45279'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:37164'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:38439'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:35018'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:35792'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:35674'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:34325'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:42523'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:34359'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:34311'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:37298'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:45245'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:37426'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:41276'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:36264'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:33794'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.23:45609'
distributed.dask_worker - INFO - End worker
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
