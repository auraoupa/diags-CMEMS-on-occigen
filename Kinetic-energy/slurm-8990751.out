/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:43607'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:37966'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:36413'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:35933'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:36106'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:34233'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:44511'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:33875'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:41094'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:35965'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:36432'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:44115'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:42632'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:33691'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:34331'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:42788'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:36261'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:39368'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:33529'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:43498'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:38572'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:43651'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:38069'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:36519'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:36128'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:42984'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:40440'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.46:45357'
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:43232
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:40504
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:35374
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:43232
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:40504
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:35374
distributed.worker - INFO -              nanny at:          172.30.6.46:37966
distributed.worker - INFO -              nanny at:          172.30.6.46:36519
distributed.worker - INFO -              bokeh at:          172.30.6.46:44551
distributed.worker - INFO -              bokeh at:          172.30.6.46:40500
distributed.worker - INFO -              nanny at:          172.30.6.46:35933
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -              bokeh at:          172.30.6.46:41334
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6097ou2n
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-gy4x1l8z
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-jctjqc49
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:34762
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:34762
distributed.worker - INFO -              nanny at:          172.30.6.46:36128
distributed.worker - INFO -              bokeh at:          172.30.6.46:45451
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-yqs5ujbi
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:44522
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:44522
distributed.worker - INFO -              nanny at:          172.30.6.46:33875
distributed.worker - INFO -              bokeh at:          172.30.6.46:36096
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-yseinr_m
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:37242
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:37242
distributed.worker - INFO -              nanny at:          172.30.6.46:33691
distributed.worker - INFO -              bokeh at:          172.30.6.46:44127
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-j5ss4rrm
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:44548
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:42463
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:44548
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:42463
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:45483
distributed.worker - INFO -              nanny at:          172.30.6.46:36432
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:45483
distributed.worker - INFO -              nanny at:          172.30.6.46:43651
distributed.worker - INFO -              bokeh at:          172.30.6.46:34973
distributed.worker - INFO -              nanny at:          172.30.6.46:45357
distributed.worker - INFO -              bokeh at:          172.30.6.46:42746
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -              bokeh at:          172.30.6.46:33059
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-mbj22k0d
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-sor5xxqi
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-f9j7wxzc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:46602
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:46602
distributed.worker - INFO -              nanny at:          172.30.6.46:36413
distributed.worker - INFO -              bokeh at:          172.30.6.46:34060
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-dq_221t5
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:35155
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:35155
distributed.worker - INFO -              nanny at:          172.30.6.46:36106
distributed.worker - INFO -              bokeh at:          172.30.6.46:40417
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-bxnm1ugx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:35640
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:35640
distributed.worker - INFO -              nanny at:          172.30.6.46:38069
distributed.worker - INFO -              bokeh at:          172.30.6.46:45343
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-a9ptisgt
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:35160
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:35160
distributed.worker - INFO -              nanny at:          172.30.6.46:34331
distributed.worker - INFO -              bokeh at:          172.30.6.46:43261
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-jo4jx6my
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:33041
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:33041
distributed.worker - INFO -              nanny at:          172.30.6.46:42788
distributed.worker - INFO -              bokeh at:          172.30.6.46:35398
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-g1ur_vv7
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:37766
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:37766
distributed.worker - INFO -              nanny at:          172.30.6.46:40440
distributed.worker - INFO -              bokeh at:          172.30.6.46:46051
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:38762
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:38762
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -              nanny at:          172.30.6.46:42984
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-mmobufo2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:          172.30.6.46:41458
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-t57m5oaa
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:43019
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:43019
distributed.worker - INFO -              nanny at:          172.30.6.46:38572
distributed.worker - INFO -              bokeh at:          172.30.6.46:33365
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-fdz1cn9y
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:42287
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:42287
distributed.worker - INFO -              nanny at:          172.30.6.46:43607
distributed.worker - INFO -              bokeh at:          172.30.6.46:42948
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-4w0z7oa5
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:36615
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:36615
distributed.worker - INFO -              nanny at:          172.30.6.46:42632
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:41903
distributed.worker - INFO -              bokeh at:          172.30.6.46:45359
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:41903
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -              nanny at:          172.30.6.46:41094
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:          172.30.6.46:33153
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0i6ub9t7
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pcu3h3qm
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:45560
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:45560
distributed.worker - INFO -              nanny at:          172.30.6.46:43498
distributed.worker - INFO -              bokeh at:          172.30.6.46:42536
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-klox31k1
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:39960
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:39960
distributed.worker - INFO -              nanny at:          172.30.6.46:36261
distributed.worker - INFO -              bokeh at:          172.30.6.46:45550
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-b3l2tgaz
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:46566
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:46566
distributed.worker - INFO -              nanny at:          172.30.6.46:33529
distributed.worker - INFO -              bokeh at:          172.30.6.46:41671
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:36592
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:36592
distributed.worker - INFO -              nanny at:          172.30.6.46:34233
distributed.worker - INFO -              bokeh at:          172.30.6.46:38992
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-gzesozq0
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-z5o2njvu
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:35776
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:35776
distributed.worker - INFO -              nanny at:          172.30.6.46:44115
distributed.worker - INFO -              bokeh at:          172.30.6.46:37776
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ak3ryj71
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:46762
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:46762
distributed.worker - INFO -              nanny at:          172.30.6.46:39368
distributed.worker - INFO -              bokeh at:          172.30.6.46:38832
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-awdgyrnt
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:34286
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:34286
distributed.worker - INFO -              nanny at:          172.30.6.46:44511
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.46:43133
distributed.worker - INFO -          Listening to:    tcp://172.30.6.46:43133
distributed.worker - INFO -              nanny at:          172.30.6.46:35965
distributed.worker - INFO -              bokeh at:          172.30.6.46:43036
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -              bokeh at:          172.30.6.46:40730
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-wu8_u5vu
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-aipnzpaq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.utils_perf - INFO - full garbage collection released 14.04 MB from 2337 reference cycles (threshold: 10.00 MB)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36514 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36772 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36346 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36814 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36700 remote=tcp://172.30.100.1:42394>
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b6bf47af208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b6bf47af208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b6bf47af208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36342 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36708 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37088 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36656 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36774 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36812 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36702 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36818 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37012 remote=tcp://172.30.100.1:42394>
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b6bf47af208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36776 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37374 remote=tcp://172.30.100.1:42394>
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b6bf47af208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37092 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36660 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36982 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37090 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37792 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37794 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37560 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36784 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37864 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37854 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37870 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37866 remote=tcp://172.30.100.1:42394>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37856 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37678 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37006 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37062 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38068 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37744 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37104 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37760 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37146 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37904 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38012 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37456 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37512 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37650 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37724 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36476 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36640 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37862 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36866 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37916 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36912 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37692 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37072 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37356 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37468 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37526 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37580 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37632 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37810 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36862 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37914 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37076 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37354 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37466 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37522 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37578 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37630 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37796 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36848 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37674 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37004 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38008 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38066 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37742 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37100 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37758 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37142 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37196 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37848 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37244 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37902 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37344 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37396 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37454 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37508 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37566 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37618 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37956 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36350 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36988 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37714 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37042 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38050 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37740 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38104 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37132 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37788 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37182 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37834 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37226 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37942 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37300 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37380 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37438 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37492 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37550 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37602 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37648 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37984 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36974 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37702 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37030 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38036 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37734 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37086 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37774 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37168 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37872 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37270 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37928 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37282 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37368 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37424 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37480 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37536 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37592 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37640 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37822 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38276 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38278 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38274 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38576 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38280 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38604 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38666 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38786 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37620 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36850 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37676 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37002 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38064 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37102 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37144 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37798 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37194 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37846 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37243 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37900 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37342 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38010 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37452 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37510 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37564 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37958 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38766 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38334 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38822 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38386 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38434 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38490 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38546 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38602 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38658 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38710 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38254 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36480 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37636 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36872 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37924 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36916 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37696 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37028 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37078 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37116 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37770 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37164 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37360 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37420 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37474 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37534 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37590 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37812 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37976 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38306 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38840 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38400 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38884 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38458 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38936 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38512 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38564 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38626 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38680 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38734 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38268 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:39446 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:39554 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:39606 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:39286 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:39314 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:39404 remote=tcp://172.30.100.1:42394>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:36846 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38006 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38062 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37192 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37240 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37394 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37450 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37506 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37562 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37616 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:37954 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38654 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38820 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38542 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38598 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:38708 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.46:39692 remote=tcp://172.30.100.1:42394>
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:43133
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:33041
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:45483
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:40504
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:35776
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:39960
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:34762
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:36592
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:43019
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:36615
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:41903
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:43232
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:45560
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:42287
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:46566
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:44522
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:35160
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:38762
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:46602
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:46762
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:37242
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:37766
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:34286
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:44548
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:42463
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:35374
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:35640
distributed.worker - INFO - Stopping worker at tcp://172.30.6.46:35155
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:34233'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:35965'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:36261'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:38069'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:33529'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:37966'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:42984'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:36106'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:45357'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:44115'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:36128'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:36432'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:40440'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:36413'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:38572'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:33875'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:42788'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:42632'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:39368'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:43651'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:43607'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:36519'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:44511'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:34331'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:43498'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:41094'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:35933'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.46:33691'
distributed.dask_worker - INFO - End worker
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
