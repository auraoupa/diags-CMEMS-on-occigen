/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:40064'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:36580'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:43228'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:34602'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:40646'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:41531'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:45596'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:44229'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:44204'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:39236'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:43361'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:35123'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:42429'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:44402'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:34904'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:45043'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:37388'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:38697'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:43836'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:38652'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:39490'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:41871'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:37571'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:35675'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:39681'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:33946'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:34728'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.238:46321'
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:33404
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:33404
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:36976
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:33998
distributed.worker - INFO -              nanny at:         172.30.8.238:46321
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:36976
distributed.worker - INFO -              bokeh at:         172.30.8.238:32933
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:33998
distributed.worker - INFO -              nanny at:         172.30.8.238:37571
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -              bokeh at:         172.30.8.238:36827
distributed.worker - INFO -              nanny at:         172.30.8.238:40064
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:34480
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -              bokeh at:         172.30.8.238:34766
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:34480
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -              nanny at:         172.30.8.238:43361
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6bjhb4g_
distributed.worker - INFO -              bokeh at:         172.30.8.238:40516
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-js6zgvkl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-kih5e7ch
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1k_cti3x
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:36680
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:36680
distributed.worker - INFO -              nanny at:         172.30.8.238:33946
distributed.worker - INFO -              bokeh at:         172.30.8.238:38353
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-z7zasgcu
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:40590
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:40590
distributed.worker - INFO -              nanny at:         172.30.8.238:34602
distributed.worker - INFO -              bokeh at:         172.30.8.238:34608
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-a3_xh9ch
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:39775
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:39775
distributed.worker - INFO -              nanny at:         172.30.8.238:45043
distributed.worker - INFO -              bokeh at:         172.30.8.238:37332
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-d1kbsn2n
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:37381
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:37381
distributed.worker - INFO -              nanny at:         172.30.8.238:45596
distributed.worker - INFO -              bokeh at:         172.30.8.238:43455
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-za3fcqc1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:38321
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:41617
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:38321
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:41617
distributed.worker - INFO -              nanny at:         172.30.8.238:35675
distributed.worker - INFO -              bokeh at:         172.30.8.238:36870
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              nanny at:         172.30.8.238:41871
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -              bokeh at:         172.30.8.238:34960
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pczskuco
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-mpsakv8q
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:36328
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:36328
distributed.worker - INFO -              nanny at:         172.30.8.238:34728
distributed.worker - INFO -              bokeh at:         172.30.8.238:35662
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9p3g4e4y
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:45679
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:45679
distributed.worker - INFO -              nanny at:         172.30.8.238:37388
distributed.worker - INFO -              bokeh at:         172.30.8.238:37678
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-m767oant
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:42876
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:42876
distributed.worker - INFO -              nanny at:         172.30.8.238:44204
distributed.worker - INFO -              bokeh at:         172.30.8.238:43741
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ugyl6dh6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:38780
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:38780
distributed.worker - INFO -              nanny at:         172.30.8.238:43228
distributed.worker - INFO -              bokeh at:         172.30.8.238:38462
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0s1evyqa
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:42760
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:42760
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO -              nanny at:         172.30.8.238:42429
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:         172.30.8.238:34845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.core - INFO - Starting established connection
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-7w56qhp4
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:33611
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:33611
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:37661
distributed.worker - INFO -              nanny at:         172.30.8.238:41531
distributed.worker - INFO -              bokeh at:         172.30.8.238:35019
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:37661
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              nanny at:         172.30.8.238:35123
distributed.worker - INFO -              bokeh at:         172.30.8.238:45714
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-rrxnjue8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-cuadupjz
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:36398
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:38505
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:36398
distributed.worker - INFO -              nanny at:         172.30.8.238:34904
distributed.worker - INFO -              bokeh at:         172.30.8.238:44095
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:38505
distributed.worker - INFO -              nanny at:         172.30.8.238:44402
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:34942
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -              bokeh at:         172.30.8.238:36408
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:34942
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-5f711fo0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              nanny at:         172.30.8.238:38697
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:         172.30.8.238:38995
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-se10336t
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_pqxgw43
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:32900
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:32900
distributed.worker - INFO -              nanny at:         172.30.8.238:44229
distributed.worker - INFO -              bokeh at:         172.30.8.238:42042
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:37837
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:38331
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:37837
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:38331
distributed.worker - INFO -              nanny at:         172.30.8.238:39490
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-jnwx5ll6
distributed.worker - INFO -              nanny at:         172.30.8.238:39236
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:         172.30.8.238:41914
distributed.worker - INFO -              bokeh at:         172.30.8.238:46387
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9h7l3b4s
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-5dfvuiux
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:33728
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:33728
distributed.worker - INFO -              nanny at:         172.30.8.238:36580
distributed.worker - INFO -              bokeh at:         172.30.8.238:41001
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-exz_dqjz
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:45777
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:45777
distributed.worker - INFO -              nanny at:         172.30.8.238:39681
distributed.worker - INFO -              bokeh at:         172.30.8.238:40130
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-qbocrvyc
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:33198
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:33198
distributed.worker - INFO -              nanny at:         172.30.8.238:40646
distributed.worker - INFO -              bokeh at:         172.30.8.238:34134
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-3wrp5_4b
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:40419
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:40419
distributed.worker - INFO -              nanny at:         172.30.8.238:43836
distributed.worker - INFO -              bokeh at:         172.30.8.238:34714
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-dm4d8a3v
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.238:40328
distributed.worker - INFO -          Listening to:   tcp://172.30.8.238:40328
distributed.worker - INFO -              nanny at:         172.30.8.238:38652
distributed.worker - INFO -              bokeh at:         172.30.8.238:45842
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-05fik_px
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 61.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 37.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:44802 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:44804 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:44808 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:44806 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50586 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50592 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50594 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50590 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50626 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50624 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50584 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50596 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50598 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50602 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50588 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50642 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50628 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50604 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50606 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49580 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49606 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49284 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49610 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49724 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49288 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49726 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49290 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49730 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49294 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49588 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49608 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49582 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49812 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49360 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49822 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49816 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49848 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49402 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49806 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49870 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49818 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49868 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49728 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49840 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49898 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49802 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49914 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49586 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49814 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49358 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49416 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49854 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49404 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49460 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49968 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49520 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49576 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49632 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50120 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49688 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49740 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49864 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49414 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49918 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49470 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49532 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50030 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49638 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49698 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49752 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50983 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50958 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51068 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51400 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51330 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50980 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51340 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51512 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51326 remote=tcp://172.30.100.1:40975>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51334 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51342 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51746 remote=tcp://172.30.100.1:40975>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51772 remote=tcp://172.30.100.1:40975>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51510 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51850 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51942 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51852 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51902 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51944 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51854 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51900 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51940 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49808 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49364 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49862 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49420 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49922 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49474 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49526 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50038 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49642 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49700 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49756 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51908 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51476 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50588 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51952 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50704 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:52062 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51640 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50756 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51694 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50814 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51756 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51808 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50928 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51862 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51098 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:52010 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51148 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51206 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51264 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51318 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51360 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51420 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49850 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49400 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49958 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49510 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49566 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49624 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50114 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49678 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49738 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50962 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50518 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50572 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51934 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50628 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51990 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51566 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:52044 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51622 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50738 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:52100 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51678 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50796 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:52158 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51732 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51788 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50908 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51844 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51898 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51018 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51078 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51130 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51188 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51242 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51300 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51456 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49590 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49810 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49362 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49865 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49422 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49926 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49478 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49530 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50036 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49644 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49702 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49754 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51910 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51472 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50594 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51954 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50702 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:52064 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51644 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50754 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51700 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50818 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51752 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51812 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50924 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51866 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51094 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:52006 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51154 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51208 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51262 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51322 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51362 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51416 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51532 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51586 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49846 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49398 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49960 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49512 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50016 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49568 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49626 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50116 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49682 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49736 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50516 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50570 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51936 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50626 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51986 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51564 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:52042 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51620 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50740 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:52098 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51676 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50794 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:52154 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51734 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51792 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50906 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51846 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51894 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51020 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51074 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51134 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51186 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51244 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51298 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51454 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51508 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49476 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49528 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50034 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49646 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49696 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49750 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51906 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51468 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50592 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51948 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50698 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:52058 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51638 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50758 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51698 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50812 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51748 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51810 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50922 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51860 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51096 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:52002 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51152 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51210 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51260 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51314 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51358 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51412 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51526 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51580 remote=tcp://172.30.100.1:40975>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49936 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49482 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49986 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49540 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49596 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50092 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49648 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49710 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:49760 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51324 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50432 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:52018 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51594 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50712 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:52070 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51650 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50770 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:52132 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51706 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50820 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50876 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51820 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50936 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:50988 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51044 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51106 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51156 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51214 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51272 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51366 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51430 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51486 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:51536 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:53270 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:52836 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:52878 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:53192 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:53272 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:52834 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:52978 remote=tcp://172.30.100.1:40975>
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:52940 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:53206 remote=tcp://172.30.100.1:40975>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:52938 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.238:53052 remote=tcp://172.30.100.1:40975>
distributed.utils_perf - INFO - full garbage collection released 21.73 MB from 5679 reference cycles (threshold: 10.00 MB)
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.utils_perf - INFO - full garbage collection released 61.37 MB from 10683 reference cycles (threshold: 10.00 MB)
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:38505
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:40419
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:42876
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:37381
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:33404
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:36976
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:37661
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:32900
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:34942
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:33998
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:40590
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:40328
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:34480
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:36398
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:45679
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:42760
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:33728
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:33198
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:37837
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:38331
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:39775
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:33611
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:36328
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:38780
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:36680
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:41617
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:38321
distributed.worker - INFO - Stopping worker at tcp://172.30.8.238:45777
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:44402'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:35123'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:34728'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:46321'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:34904'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:38652'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:44204'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:40064'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:37571'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:42429'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:38697'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:39490'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:45043'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:43836'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:45596'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:41531'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:41871'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:43228'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:39236'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:34602'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:33946'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:36580'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:37388'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:43361'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:35675'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:44229'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:40646'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.238:39681'
distributed.dask_worker - INFO - End worker
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
