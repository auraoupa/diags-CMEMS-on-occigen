/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:34037'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:33160'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:42389'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:46659'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:45680'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:35834'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:36839'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:42655'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:39739'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:38164'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:41511'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:36454'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:36124'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:44819'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:40511'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:38311'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:39308'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:36471'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:40868'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:40642'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:38426'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:38120'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:34799'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:44061'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:37652'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:33928'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:45117'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.134:45623'
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:45319
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:34962
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:45319
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:34962
distributed.worker - INFO -              nanny at:         172.30.9.134:46659
distributed.worker - INFO -              nanny at:         172.30.9.134:39308
distributed.worker - INFO -              bokeh at:         172.30.9.134:37680
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:39723
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:39723
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:         172.30.9.134:39123
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -              nanny at:         172.30.9.134:36839
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -              bokeh at:         172.30.9.134:34833
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-5jrb9hdu
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0lu93wd6
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-4dqi7ur9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:43034
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:43034
distributed.worker - INFO -              nanny at:         172.30.9.134:34799
distributed.worker - INFO -              bokeh at:         172.30.9.134:45275
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ga91l9g2
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:46836
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:46836
distributed.worker - INFO -              nanny at:         172.30.9.134:45117
distributed.worker - INFO -              bokeh at:         172.30.9.134:33031
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-xyw0njn4
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:35575
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:35575
distributed.worker - INFO -              nanny at:         172.30.9.134:40511
distributed.worker - INFO -              bokeh at:         172.30.9.134:36374
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-owdev6n6
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:38546
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:38546
distributed.worker - INFO -              nanny at:         172.30.9.134:34037
distributed.worker - INFO -              bokeh at:         172.30.9.134:43520
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:41098
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:41098
distributed.worker - INFO -                Memory:                    4.29 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:35567
distributed.worker - INFO -              nanny at:         172.30.9.134:45623
distributed.worker - INFO -              bokeh at:         172.30.9.134:34086
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:35567
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO -              nanny at:         172.30.9.134:40642
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-nni9ylvz
distributed.worker - INFO -              bokeh at:         172.30.9.134:36401
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-w4lr1rld
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-yx6ewzyi
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:34104
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:34104
distributed.worker - INFO -              nanny at:         172.30.9.134:41511
distributed.worker - INFO -              bokeh at:         172.30.9.134:36806
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ziwlrcdh
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:34927
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:34927
distributed.worker - INFO -              nanny at:         172.30.9.134:36124
distributed.worker - INFO -              bokeh at:         172.30.9.134:32842
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-svmp3jkc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:38303
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:38303
distributed.worker - INFO -              nanny at:         172.30.9.134:37652
distributed.worker - INFO -              bokeh at:         172.30.9.134:34819
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-r3izxs7n
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:41019
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:41019
distributed.worker - INFO -              nanny at:         172.30.9.134:38311
distributed.worker - INFO -              bokeh at:         172.30.9.134:43172
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-nlx8zjla
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:38254
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:38254
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:45517
distributed.worker - INFO -              nanny at:         172.30.9.134:44061
distributed.worker - INFO -              bokeh at:         172.30.9.134:46720
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:45517
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO -              nanny at:         172.30.9.134:35834
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:         172.30.9.134:46785
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-oe8et44p
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-7wv3jbzi
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:46604
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:46604
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:43737
distributed.worker - INFO -              nanny at:         172.30.9.134:38426
distributed.worker - INFO -              bokeh at:         172.30.9.134:34595
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:43737
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:40486
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              nanny at:         172.30.9.134:45680
distributed.worker - INFO -              bokeh at:         172.30.9.134:41764
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:40486
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -              nanny at:         172.30.9.134:33160
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-m75ws9ul
distributed.worker - INFO -              bokeh at:         172.30.9.134:46794
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:38869
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:38869
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:46695
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -              nanny at:         172.30.9.134:36471
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:46695
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO -              bokeh at:         172.30.9.134:40483
distributed.worker - INFO -              nanny at:         172.30.9.134:38164
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-fcw8vy3e
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO -              bokeh at:         172.30.9.134:34350
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ag9w7s3i
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-n25b5q_w
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-8lq29xx3
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:45561
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:35432
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:45561
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:44881
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:35432
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:45617
distributed.worker - INFO -              nanny at:         172.30.9.134:40868
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:44881
distributed.worker - INFO -              nanny at:         172.30.9.134:33928
distributed.worker - INFO -              bokeh at:         172.30.9.134:36241
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:38181
distributed.worker - INFO -              nanny at:         172.30.9.134:42655
distributed.worker - INFO -              bokeh at:         172.30.9.134:36923
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:45617
distributed.worker - INFO -              bokeh at:         172.30.9.134:41245
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:38181
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              nanny at:         172.30.9.134:42389
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO -              nanny at:         172.30.9.134:44819
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:         172.30.9.134:37488
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:         172.30.9.134:41911
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6cvf3uqu
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-qq8gra42
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-terge9d2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-jst_ov8h
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-cgvs2oiz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:45228
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:45228
distributed.worker - INFO -              nanny at:         172.30.9.134:38120
distributed.worker - INFO -              bokeh at:         172.30.9.134:46542
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-gsdt161_
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:41512
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:41512
distributed.worker - INFO -              nanny at:         172.30.9.134:39739
distributed.worker - INFO -              bokeh at:         172.30.9.134:34470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.9.134:40740
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          Listening to:   tcp://172.30.9.134:40740
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-kch3km54
distributed.worker - INFO -              nanny at:         172.30.9.134:36454
distributed.worker - INFO -              bokeh at:         172.30.9.134:43140
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ectmyq5c
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:44717
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35194 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35230 remote=tcp://172.30.100.2:44717>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35240 remote=tcp://172.30.100.2:44717>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35192 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35228 remote=tcp://172.30.100.2:44717>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35116 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35224 remote=tcp://172.30.100.2:44717>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35190 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35226 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35118 remote=tcp://172.30.100.2:44717>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35222 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35256 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35112 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35186 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35596 remote=tcp://172.30.100.2:44717>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35088 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35124 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35160 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35630 remote=tcp://172.30.100.2:44717>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35218 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35254 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35290 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35312 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35110 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35146 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35182 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35594 remote=tcp://172.30.100.2:44717>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35216 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35252 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35288 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35108 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35144 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35180 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35592 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35086 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35122 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35158 remote=tcp://172.30.100.2:44717>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35092 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35128 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35634 remote=tcp://172.30.100.2:44717>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35238 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35130 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35202 remote=tcp://172.30.100.2:44717>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35250 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35106 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35142 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35214 remote=tcp://172.30.100.2:44717>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35220 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35258 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35114 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35184 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35598 remote=tcp://172.30.100.2:44717>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35090 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35126 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35632 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35084 remote=tcp://172.30.100.2:44717>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35242 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35098 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35134 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35206 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35580 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35638 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35120 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35156 remote=tcp://172.30.100.2:44717>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35138 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35176 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35248 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35590 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35096 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35132 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35170 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35204 remote=tcp://172.30.100.2:44717>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35140 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35174 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35246 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35588 remote=tcp://172.30.100.2:44717>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35136 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35172 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35244 remote=tcp://172.30.100.2:44717>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.134:35586 remote=tcp://172.30.100.2:44717>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:44717
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:43737
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:45617
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:45561
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:35575
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:39723
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:40740
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:41098
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:45680'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:42389'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:36839'
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:34962
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:45228
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:46695
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:38546
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:40868'
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:34104
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:38303
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:46836
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:45623'
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:43034
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:45517
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:35567
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:41512
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:36454'
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:41019
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:45319
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:38254
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:44881
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:38869
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:35432
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:34927
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:46604
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:38120'
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:40486
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:34037'
distributed.worker - INFO - Stopping worker at tcp://172.30.9.134:38181
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:38164'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:45117'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:37652'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:35834'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:34799'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:41511'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:38311'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:46659'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:40642'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:44061'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:36471'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:42655'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:33928'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:36124'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:38426'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:44819'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:33160'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:39739'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:40511'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.134:39308'
distributed.dask_worker - INFO - End worker
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
