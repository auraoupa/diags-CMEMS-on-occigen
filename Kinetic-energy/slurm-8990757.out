/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:41794'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:41387'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:36143'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:38008'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:43846'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:44466'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:43200'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:34535'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:37462'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:34809'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:38070'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:36183'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:36138'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:45414'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:36787'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:35632'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:36448'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:34517'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:40386'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:43475'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:36077'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:43852'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:35526'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:39354'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:39756'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:42905'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:43674'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.9.82:45090'
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:39680
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:39680
distributed.worker - INFO -              nanny at:          172.30.9.82:41794
distributed.worker - INFO -              bokeh at:          172.30.9.82:36039
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-s915ttk_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:40665
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:40665
distributed.worker - INFO -              nanny at:          172.30.9.82:39354
distributed.worker - INFO -              bokeh at:          172.30.9.82:37355
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-uiz20ufq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:37712
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:37712
distributed.worker - INFO -              nanny at:          172.30.9.82:36143
distributed.worker - INFO -              bokeh at:          172.30.9.82:33420
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-23b4c8nu
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:43636
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:43636
distributed.worker - INFO -              nanny at:          172.30.9.82:34535
distributed.worker - INFO -              bokeh at:          172.30.9.82:38916
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_y25ddl9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:45524
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:45524
distributed.worker - INFO -              nanny at:          172.30.9.82:34517
distributed.worker - INFO -              bokeh at:          172.30.9.82:36374
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-4yahxqn8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:34955
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:43754
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:34955
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:43754
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:45933
distributed.worker - INFO -              nanny at:          172.30.9.82:42905
distributed.worker - INFO -              nanny at:          172.30.9.82:37462
distributed.worker - INFO -              bokeh at:          172.30.9.82:39741
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:45933
distributed.worker - INFO -              bokeh at:          172.30.9.82:34178
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -              nanny at:          172.30.9.82:38070
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:          172.30.9.82:40894
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-rzi9t1r8
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-wws9bdgc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-w2zg7c4c
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:45658
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:45658
distributed.worker - INFO -              nanny at:          172.30.9.82:43852
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:39239
distributed.worker - INFO -              bokeh at:          172.30.9.82:44612
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:39239
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              nanny at:          172.30.9.82:36138
distributed.worker - INFO -              bokeh at:          172.30.9.82:43257
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1l12shw0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-kucy4cy3
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:39463
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:39463
distributed.worker - INFO -              nanny at:          172.30.9.82:36448
distributed.worker - INFO -              bokeh at:          172.30.9.82:37425
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-qtfu79wi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:33872
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:33872
distributed.worker - INFO -              nanny at:          172.30.9.82:39756
distributed.worker - INFO -              bokeh at:          172.30.9.82:45890
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-dowutfmz
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:39577
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:39577
distributed.worker - INFO -              nanny at:          172.30.9.82:34809
distributed.worker - INFO -              bokeh at:          172.30.9.82:45518
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-11t8utf1
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:44543
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:44543
distributed.worker - INFO -              nanny at:          172.30.9.82:45090
distributed.worker - INFO -              bokeh at:          172.30.9.82:43977
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-uxo7x3sb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:42171
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:42171
distributed.worker - INFO -              nanny at:          172.30.9.82:35632
distributed.worker - INFO -              bokeh at:          172.30.9.82:39415
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-y_td9ddc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:33685
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:33685
distributed.worker - INFO -              nanny at:          172.30.9.82:43674
distributed.worker - INFO -              bokeh at:          172.30.9.82:46537
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-5w77qlf8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:38153
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:38153
distributed.worker - INFO -              nanny at:          172.30.9.82:43200
distributed.worker - INFO -              bokeh at:          172.30.9.82:42557
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:36650
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:39143
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:39143
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              nanny at:          172.30.9.82:43846
distributed.worker - INFO -              bokeh at:          172.30.9.82:41454
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:36650
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-vkx804r3
distributed.worker - INFO -              nanny at:          172.30.9.82:44466
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:          172.30.9.82:43252
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-nj478afi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-bn9w76xa
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:41757
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:41757
distributed.worker - INFO -              nanny at:          172.30.9.82:36077
distributed.worker - INFO -              bokeh at:          172.30.9.82:35413
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-7murkg54
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:43205
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:43205
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:40251
distributed.worker - INFO -              nanny at:          172.30.9.82:36787
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:40251
distributed.worker - INFO -              nanny at:          172.30.9.82:41387
distributed.worker - INFO -              bokeh at:          172.30.9.82:39126
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:44452
distributed.worker - INFO -              bokeh at:          172.30.9.82:40947
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:44452
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -              nanny at:          172.30.9.82:45414
distributed.worker - INFO -              bokeh at:          172.30.9.82:34434
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-gcawv1bj
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-64owmis0
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-82ron4ur
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:37135
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:37135
distributed.worker - INFO -              nanny at:          172.30.9.82:43475
distributed.worker - INFO -              bokeh at:          172.30.9.82:35816
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-f0nlfcld
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:41476
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:41476
distributed.worker - INFO -              nanny at:          172.30.9.82:38008
distributed.worker - INFO -              bokeh at:          172.30.9.82:45669
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:41305
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:41305
distributed.worker - INFO -              nanny at:          172.30.9.82:36183
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -              bokeh at:          172.30.9.82:34319
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ymg2k53a
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-bh7udlwm
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:40317
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:40317
distributed.worker - INFO -              nanny at:          172.30.9.82:35526
distributed.worker - INFO -              bokeh at:          172.30.9.82:41327
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-h3o__i14
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.9.82:42325
distributed.worker - INFO -          Listening to:    tcp://172.30.9.82:42325
distributed.worker - INFO -              nanny at:          172.30.9.82:40386
distributed.worker - INFO -              bokeh at:          172.30.9.82:41750
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-709qre1p
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:42394
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.utils_perf - INFO - full garbage collection released 11.15 MB from 281 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49050 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49000 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:48998 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49394 remote=tcp://172.30.100.1:42394>
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ac2151b1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ac2151b1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ac2151b1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ac2151b1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ac2151b1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ac2151b1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ac2151b1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ac2151b1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ac2151b1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ac2151b1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ac2151b1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ac2151b1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:48968 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49060 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49298 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49164 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49804 remote=tcp://172.30.100.1:42394>
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ac2151b1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2ac2151b1208>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 748, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.1:42394' after 10 s: connect() didn't finish in time
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:48960 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49802 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49460 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50074 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49712 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50076 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:48964 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49272 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49480 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:48942 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49876 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49794 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49714 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50224 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:48944 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49114 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49226 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49808 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49974 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49242 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50302 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50226 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49806 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49972 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50360 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50385 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50306 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50426 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50392 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49228 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50016 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49338 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:48930 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50440 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50082 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49384 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50390 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50448 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50088 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50116 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50168 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49476 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50218 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49528 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50246 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49566 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50294 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49618 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50338 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49702 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49732 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49824 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49888 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49946 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50002 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50384 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50438 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50084 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49385 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50120 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50174 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49472 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50214 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49522 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50244 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49562 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50290 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49622 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50340 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49708 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49754 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49828 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49882 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49936 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49994 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50042 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49896 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49162 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50126 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50182 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49482 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50546 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49660 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50454 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49746 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49840 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50008 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50094 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49230 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50018 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49342 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50138 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49442 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50502 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50558 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50230 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49540 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50256 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49584 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49640 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50354 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49672 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50410 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49848 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49906 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49964 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50062 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50464 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:48962 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49968 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50064 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49344 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50412 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50466 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49402 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50140 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49444 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50190 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49498 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50234 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50258 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49588 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49770 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49858 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49914 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50022 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50308 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50418 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50474 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50102 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49410 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50146 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49452 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50510 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50192 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49504 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50264 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50364 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49682 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49718 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49862 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49916 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49976 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50030 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50316 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50382 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50436 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50482 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50490 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50164 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49464 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50206 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49516 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50242 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49560 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50278 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49606 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50330 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49656 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49698 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49728 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49780 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49820 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49878 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49930 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49986 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50378 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50434 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50478 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50108 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50160 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50204 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49518 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50282 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49608 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50332 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49654 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49694 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49738 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49778 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49818 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49934 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49990 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49966 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50358 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50068 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49346 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50414 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50470 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49406 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50144 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49448 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49496 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50232 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50260 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49590 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49854 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49912 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50026 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50304 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50020 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49340 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50136 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49440 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50498 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50554 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49542 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50254 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49582 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49638 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50356 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49670 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50408 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49850 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49908 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49962 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50058 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50460 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50222 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50450 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50090 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50124 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49428 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50178 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49532 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50250 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49574 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50296 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49624 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50348 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50398 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49740 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49838 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49894 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49950 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50006 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50422 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50476 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49408 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50150 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49450 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50514 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49506 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50268 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49596 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50368 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49684 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49720 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49864 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49918 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50032 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50072 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50318 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49970 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50362 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50066 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49348 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50416 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50468 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49404 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50141 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49446 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49499 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50262 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49586 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49766 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49856 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49910 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50024 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50838 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50832 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50840 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50844 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51134 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50836 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51082 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51084 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51080 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50842 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51114 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50834 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51096 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51106 remote=tcp://172.30.100.1:42394>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51132 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51108 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51110 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51168 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51130 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51822 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51938 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51926 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51928 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51936 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51924 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51820 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50374 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50430 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50486 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50156 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50202 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49554 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50276 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49604 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50324 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49692 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49724 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49774 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49816 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49872 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49928 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:49982 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50040 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51244 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51302 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50874 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51358 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:50924 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51026 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51078 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51188 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51400 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:51808 remote=tcp://172.30.100.1:42394>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.9.82:52146 remote=tcp://172.30.100.1:42394>
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.utils_perf - INFO - full garbage collection released 39.35 MB from 4679 reference cycles (threshold: 10.00 MB)
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.utils_perf - INFO - full garbage collection released 28.04 MB from 5923 reference cycles (threshold: 10.00 MB)
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:42394
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:39577
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:34955
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:39143
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:42171
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:40317
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:43636
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:43205
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:45933
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:45524
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:42325
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:39239
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:41305
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:45658
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:40665
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:36650
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:33872
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:44452
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:37712
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:39680
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:37135
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:43754
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:44543
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:40251
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:41757
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:38153
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:41476
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:39463
distributed.worker - INFO - Stopping worker at tcp://172.30.9.82:33685
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:34809'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:42905'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:35632'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:41794'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:43852'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:35526'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:34517'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:38008'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:43846'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:36183'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:36138'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:34535'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:37462'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:36077'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:43674'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:41387'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:39354'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:39756'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:43475'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:36787'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:43200'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:44466'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:40386'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:38070'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:45090'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:36143'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:45414'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.9.82:36448'
distributed.dask_worker - INFO - End worker
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
