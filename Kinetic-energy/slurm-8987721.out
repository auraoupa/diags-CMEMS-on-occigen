/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:37116'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:41999'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:36724'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:42000'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:42492'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:35286'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:39919'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:46468'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:35673'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:45034'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:44895'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:36596'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:41940'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:34350'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:36789'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:42391'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:35343'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:46041'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:36465'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:43481'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:36838'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:42467'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:44743'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:40062'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:46384'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:36849'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:39758'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.237:34711'
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:42877
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:46490
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:43045
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:36085
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:42877
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:44799
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:46490
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:42436
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:43045
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:36085
distributed.worker - INFO -              nanny at:         172.30.8.237:35343
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:44799
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:42436
distributed.worker - INFO -              nanny at:         172.30.8.237:40062
distributed.worker - INFO -              nanny at:         172.30.8.237:44743
distributed.worker - INFO -              bokeh at:         172.30.8.237:42498
distributed.worker - INFO -              nanny at:         172.30.8.237:46041
distributed.worker - INFO -              nanny at:         172.30.8.237:34711
distributed.worker - INFO -              bokeh at:         172.30.8.237:33435
distributed.worker - INFO -              bokeh at:         172.30.8.237:37589
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:36676
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -              bokeh at:         172.30.8.237:43396
distributed.worker - INFO -              bokeh at:         172.30.8.237:43291
distributed.worker - INFO -              nanny at:         172.30.8.237:45034
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:36676
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -              bokeh at:         172.30.8.237:44563
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              nanny at:         172.30.8.237:42000
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -              bokeh at:         172.30.8.237:37016
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6m4w_15s
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-zz7k0rp8
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-kv7sjm7o
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-n2z_yyn7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-zf4oaud1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:42407
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-mndlqsye
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:42407
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9m5bg56q
distributed.worker - INFO -              nanny at:         172.30.8.237:43481
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:         172.30.8.237:37420
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:38118
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-d7t__yzh
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:38118
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              nanny at:         172.30.8.237:39758
distributed.worker - INFO -              bokeh at:         172.30.8.237:35303
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-fatzfc00
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:35990
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:35990
distributed.worker - INFO -              nanny at:         172.30.8.237:36596
distributed.worker - INFO -              bokeh at:         172.30.8.237:34978
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-7rgj_45h
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:40947
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:40947
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:41137
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:41137
distributed.worker - INFO -              nanny at:         172.30.8.237:41940
distributed.worker - INFO -              nanny at:         172.30.8.237:35286
distributed.worker - INFO -              bokeh at:         172.30.8.237:45031
distributed.worker - INFO -              bokeh at:         172.30.8.237:35652
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-wy16hyoc
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-cu8pdljf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:36498
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:36498
distributed.worker - INFO -              nanny at:         172.30.8.237:35673
distributed.worker - INFO -              bokeh at:         172.30.8.237:41767
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-3yng2mq5
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:42727
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:42727
distributed.worker - INFO -              nanny at:         172.30.8.237:46468
distributed.worker - INFO -              bokeh at:         172.30.8.237:46767
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-sf4f2p54
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:39824
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:39824
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:40346
distributed.worker - INFO -              nanny at:         172.30.8.237:42492
distributed.worker - INFO -              bokeh at:         172.30.8.237:41834
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:40346
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -              nanny at:         172.30.8.237:36838
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1xhagkoh
distributed.worker - INFO -              bokeh at:         172.30.8.237:38728
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-qg28yhbv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:42269
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:43799
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:42269
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:43799
distributed.worker - INFO -              nanny at:         172.30.8.237:37116
distributed.worker - INFO -              nanny at:         172.30.8.237:36789
distributed.worker - INFO -              bokeh at:         172.30.8.237:34125
distributed.worker - INFO -              bokeh at:         172.30.8.237:40047
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-jhya_umg
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-twug7lxo
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:32963
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:32963
distributed.worker - INFO -              nanny at:         172.30.8.237:34350
distributed.worker - INFO -              bokeh at:         172.30.8.237:37062
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-qcegw18d
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:37632
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:37632
distributed.worker - INFO -              nanny at:         172.30.8.237:46384
distributed.worker - INFO -              bokeh at:         172.30.8.237:34464
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-thr0afg3
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:43292
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:43292
distributed.worker - INFO -              nanny at:         172.30.8.237:39919
distributed.worker - INFO -              bokeh at:         172.30.8.237:38233
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6fido02u
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:33581
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:33581
distributed.worker - INFO -              nanny at:         172.30.8.237:42467
distributed.worker - INFO -              bokeh at:         172.30.8.237:41728
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:37568
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:37568
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              nanny at:         172.30.8.237:36465
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -              bokeh at:         172.30.8.237:46043
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-25gbh5ha
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-lrth9tkv
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:36019
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:36019
distributed.worker - INFO -              nanny at:         172.30.8.237:41999
distributed.worker - INFO -              bokeh at:         172.30.8.237:36987
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_buvztyw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:32999
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:32999
distributed.worker - INFO -              nanny at:         172.30.8.237:36849
distributed.worker - INFO -              bokeh at:         172.30.8.237:43113
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-12drg23o
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:33547
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:33547
distributed.worker - INFO -              nanny at:         172.30.8.237:36724
distributed.worker - INFO -              bokeh at:         172.30.8.237:39559
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-xrlz_h0z
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:45059
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:45059
distributed.worker - INFO -              nanny at:         172.30.8.237:42391
distributed.worker - INFO -              bokeh at:         172.30.8.237:37951
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-b82pzbfq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.237:33519
distributed.worker - INFO -          Listening to:   tcp://172.30.8.237:33519
distributed.worker - INFO -              nanny at:         172.30.8.237:44895
distributed.worker - INFO -              bokeh at:         172.30.8.237:44686
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-tq3_o0rk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 37.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:50278 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:50296 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:50294 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:50298 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:50284 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:50286 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:50288 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:50292 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:50290 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46028 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46026 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46054 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46060 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46052 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46058 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46040 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46038 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46072 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46074 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46042 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46048 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46068 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46070 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46030 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46036 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46034 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46032 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46044 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46056 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46049 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46018 remote=tcp://172.30.8.67:37053>
distributed.utils_perf - INFO - full garbage collection released 11.08 MB from 591 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:50280 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:50282 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38456 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38142 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38458 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38442 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38612 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38466 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38588 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38470 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38140 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38448 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38618 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38172 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38464 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38444 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38610 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38164 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38450 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38620 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38168 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38452 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38616 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38170 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38672 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38226 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38676 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38228 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38446 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38614 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38670 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38222 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38166 remote=tcp://172.30.100.1:40975>
distributed.utils_perf - INFO - full garbage collection released 16.02 MB from 412 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38696 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38248 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38714 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38684 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38798 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38712 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39354 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39350 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40218 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46022 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:46024 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38608 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38162 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38662 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38216 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38776 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38328 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38830 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38384 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38440 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38496 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38552 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38720 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39810 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40362 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39356 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39880 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39714 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39882 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39348 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39904 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39808 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39360 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39902 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40148 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40156 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40310 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40366 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40364 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40614 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40570 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40754 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40664 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40816 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40752 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40864 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38800 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38352 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38856 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38408 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38912 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38954 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38520 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38576 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39252 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40842 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40410 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40466 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39584 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40954 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40522 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39640 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39696 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40634 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39752 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40688 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40730 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39864 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40786 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39920 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39976 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38634 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38186 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38690 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38242 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38746 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38298 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38802 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38354 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38410 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38522 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38578 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39088 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40580 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40146 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39254 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40244 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39362 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40356 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39474 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40844 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40412 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39530 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40900 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40468 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39586 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40956 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40524 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39642 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39698 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40636 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39754 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40690 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40732 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39866 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40788 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39922 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39978 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40034 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40090 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38268 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38322 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38824 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38378 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38430 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38490 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38978 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38544 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38604 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40212 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39330 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40266 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39386 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40758 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40320 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39442 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39500 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40868 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39554 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40924 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40494 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39610 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40978 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40604 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40710 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39836 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39888 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40812 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39948 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40004 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40056 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40110 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40382 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40434 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40548 remote=tcp://172.30.100.1:40975>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38218 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38668 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38724 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38778 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38330 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38388 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38500 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:38556 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39900 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39230 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39286 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40764 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40334 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39450 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40388 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40876 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40500 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39618 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40556 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39674 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39842 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:39954 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40012 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40066 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40122 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40220 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40276 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40444 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:40670 remote=tcp://172.30.100.1:40975>
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:41564 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:41990 remote=tcp://172.30.100.1:40975>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.237:41620 remote=tcp://172.30.100.1:40975>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.utils_perf - INFO - full garbage collection released 15.25 MB from 7194 reference cycles (threshold: 10.00 MB)
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.utils_perf - INFO - full garbage collection released 16.34 MB from 10483 reference cycles (threshold: 10.00 MB)
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:36019
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:43045
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:36676
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:33519
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:43292
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:42727
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:44799
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:39824
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:43799
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:37568
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:32963
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:42269
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:35990
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:33547
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:38118
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:40947
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:32999
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:36498
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:33581
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:45059
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:42877
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:37632
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:40346
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:42407
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:46490
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:36085
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:41137
distributed.worker - INFO - Stopping worker at tcp://172.30.8.237:42436
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:41999'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:40062'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:42000'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:36849'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:44895'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:39919'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:46468'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:34350'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:36789'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:46041'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:37116'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:35673'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:42492'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:36596'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:36724'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:41940'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:46384'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:39758'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:42467'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:36838'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:43481'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:45034'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:36465'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:44743'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:35286'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:34711'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:42391'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.237:35343'
distributed.dask_worker - INFO - End worker
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
