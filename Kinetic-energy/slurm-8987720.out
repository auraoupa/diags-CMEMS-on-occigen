/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:33133'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:40685'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:42283'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:36391'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:40191'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:33554'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:38480'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:40047'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:40076'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:34553'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:43271'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:44916'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:39691'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:33079'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:33922'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:39007'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:45976'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:42474'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:38305'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:42865'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:46470'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:37317'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:35558'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:39877'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:45382'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:42116'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:38853'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.64:41418'
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:40322
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:39139
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:40322
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:39139
distributed.worker - INFO -              nanny at:          172.30.6.64:45382
distributed.worker - INFO -              nanny at:          172.30.6.64:33079
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:42113
distributed.worker - INFO -              bokeh at:          172.30.6.64:33945
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:42113
distributed.worker - INFO -              bokeh at:          172.30.6.64:33898
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -              nanny at:          172.30.6.64:42116
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:          172.30.6.64:35299
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-x526ezms
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9mxy875d
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-4uuyhkhv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:41123
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:41123
distributed.worker - INFO -              nanny at:          172.30.6.64:39007
distributed.worker - INFO -              bokeh at:          172.30.6.64:46407
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-hgvwnfwk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:40637
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:40637
distributed.worker - INFO -              nanny at:          172.30.6.64:42474
distributed.worker - INFO -              bokeh at:          172.30.6.64:36866
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-aach1x4d
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:40174
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:40174
distributed.worker - INFO -              nanny at:          172.30.6.64:36391
distributed.worker - INFO -              bokeh at:          172.30.6.64:41791
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9z5injg4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:35414
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:35414
distributed.worker - INFO -              nanny at:          172.30.6.64:35558
distributed.worker - INFO -              bokeh at:          172.30.6.64:40953
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-2sra5g6b
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:40189
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:40189
distributed.worker - INFO -              nanny at:          172.30.6.64:33554
distributed.worker - INFO -              bokeh at:          172.30.6.64:44442
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-mvly7psc
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:40835
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:40835
distributed.worker - INFO -              nanny at:          172.30.6.64:38305
distributed.worker - INFO -              bokeh at:          172.30.6.64:45104
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-bsjcy7zk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:43960
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:43960
distributed.worker - INFO -              nanny at:          172.30.6.64:39877
distributed.worker - INFO -              bokeh at:          172.30.6.64:35595
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_fuglxxf
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:35618
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:35618
distributed.worker - INFO -              nanny at:          172.30.6.64:38853
distributed.worker - INFO -              bokeh at:          172.30.6.64:36205
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-x_pjw5z_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:34983
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:34983
distributed.worker - INFO -              nanny at:          172.30.6.64:46470
distributed.worker - INFO -              bokeh at:          172.30.6.64:46478
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-z8puhx79
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:46643
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:46643
distributed.worker - INFO -              nanny at:          172.30.6.64:40191
distributed.worker - INFO -              bokeh at:          172.30.6.64:41371
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9osknwk6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:42096
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:42096
distributed.worker - INFO -              nanny at:          172.30.6.64:39691
distributed.worker - INFO -              bokeh at:          172.30.6.64:42027
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.core - INFO - Starting established connection
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6n8sk3kw
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:34760
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:34760
distributed.worker - INFO -              nanny at:          172.30.6.64:42865
distributed.worker - INFO -              bokeh at:          172.30.6.64:36042
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9h_jto9o
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:36686
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:36686
distributed.worker - INFO -              nanny at:          172.30.6.64:33922
distributed.worker - INFO -              bokeh at:          172.30.6.64:44192
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-dbne807i
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:35917
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:35917
distributed.worker - INFO -              nanny at:          172.30.6.64:37317
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO -              bokeh at:          172.30.6.64:43624
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-7r3aclft
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:36786
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:36786
distributed.worker - INFO -              nanny at:          172.30.6.64:40076
distributed.worker - INFO -              bokeh at:          172.30.6.64:41178
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-87bwxwh3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:42616
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:42616
distributed.worker - INFO -              nanny at:          172.30.6.64:34553
distributed.worker - INFO -              bokeh at:          172.30.6.64:39225
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-cv4z_tgs
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:34501
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:34501
distributed.worker - INFO -              nanny at:          172.30.6.64:42283
distributed.worker - INFO -              bokeh at:          172.30.6.64:39572
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ggdeuebn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:35912
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:35912
distributed.worker - INFO -              nanny at:          172.30.6.64:40047
distributed.worker - INFO -              bokeh at:          172.30.6.64:36192
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-amyglu37
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:42593
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:42593
distributed.worker - INFO -              nanny at:          172.30.6.64:33133
distributed.worker - INFO -              bokeh at:          172.30.6.64:35450
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:36523
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:36523
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0ps975i0
distributed.worker - INFO -              nanny at:          172.30.6.64:44916
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:          172.30.6.64:36187
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-xn76yhb7
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:39329
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:39329
distributed.worker - INFO -              nanny at:          172.30.6.64:45976
distributed.worker - INFO -              bokeh at:          172.30.6.64:45200
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1_m3zs8z
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:41911
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:41911
distributed.worker - INFO -              nanny at:          172.30.6.64:38480
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO -              bokeh at:          172.30.6.64:33399
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.core - INFO - Starting established connection
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-4zjyvx31
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:45622
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:45622
distributed.worker - INFO -              nanny at:          172.30.6.64:43271
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO -              bokeh at:          172.30.6.64:37264
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.core - INFO - Starting established connection
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-mh718kru
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:37268
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:37268
distributed.worker - INFO -              nanny at:          172.30.6.64:40685
distributed.worker - INFO -              bokeh at:          172.30.6.64:41973
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-w5g2zbi9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.64:33387
distributed.worker - INFO -          Listening to:    tcp://172.30.6.64:33387
distributed.worker - INFO -              nanny at:          172.30.6.64:41418
distributed.worker - INFO -              bokeh at:          172.30.6.64:35979
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9twn2cs4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:40975
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:48592 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:48594 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:48608 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:48622 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:48624 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:48600 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:48602 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:48604 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:48606 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:48618 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:48620 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:48596 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:48598 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:48590 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:48610 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:48612 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:48614 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:48616 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53390 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53392 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53382 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53384 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53364 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53368 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53366 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53362 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53352 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53358 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53370 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53386 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53388 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53356 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53360 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53402 remote=tcp://172.30.8.67:37053>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:48626 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:48628 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:48630 remote=tcp://172.30.6.177:34860>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53556 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53684 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53254 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53548 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53716 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53678 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53680 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53250 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53554 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53722 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53552 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53720 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53284 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53276 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53550 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53718 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53282 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53772 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53326 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53786 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53784 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53814 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53800 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53856 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53278 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53766 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53324 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53880 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53432 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53858 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54898 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54444 remote=tcp://172.30.100.1:40975>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54896 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54842 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55294 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54858 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55304 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55256 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54984 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55040 remote=tcp://172.30.100.1:40975>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55046 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54836 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55298 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55254 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55714 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55324 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55716 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55668 remote=tcp://172.30.100.1:40975>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55882 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53934 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53488 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53992 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53544 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54038 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53600 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53656 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53712 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53822 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53242 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54856 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54408 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55800 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55350 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54466 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55946 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55518 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54632 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55576 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54690 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54744 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54802 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54970 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55080 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55138 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55194 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53808 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53356 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53914 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53468 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53974 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53526 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53582 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53636 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53692 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54892 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54558 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55930 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55500 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54616 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55554 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54668 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:56044 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55614 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54722 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54784 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55726 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55782 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54948 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55872 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55004 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55060 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55118 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55174 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55236 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53816 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53370 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53924 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53480 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53982 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53538 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53590 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54088 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53650 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53706 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54514 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55888 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54574 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55938 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55512 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55998 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55564 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54678 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:56056 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55622 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54734 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55678 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54794 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55732 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54852 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55792 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54910 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55834 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54966 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55020 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55074 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55186 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55242 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55338 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55400 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55456 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53802 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53358 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53860 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53916 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53970 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53528 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53578 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53690 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54838 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54562 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55924 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55498 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54614 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55560 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54670 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:56036 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55610 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54728 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55664 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54782 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55728 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55778 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55870 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55008 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55062 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55120 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55176 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55232 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55290 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55330 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55390 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55442 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53788 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53842 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53898 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53448 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53952 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53506 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53558 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53618 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53674 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55044 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55098 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54368 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55964 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55538 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54650 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:56022 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54708 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:56074 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55646 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54762 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55708 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54820 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55762 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54876 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55820 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54930 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55908 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55156 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55214 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55270 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55314 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55366 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55426 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55480 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55596 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55854 remote=tcp://172.30.100.1:40975>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53330 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53384 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53888 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53440 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53496 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53608 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53664 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54364 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54418 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54530 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55472 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54586 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55528 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:56068 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55640 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55696 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54810 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55844 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54978 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55090 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:56012 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55148 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55204 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55360 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55416 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55584 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55752 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55808 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:56720 remote=tcp://172.30.100.1:40975>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:57300 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:56866 remote=tcp://172.30.100.1:40975>
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:57238 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:56804 remote=tcp://172.30.100.1:40975>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:56800 remote=tcp://172.30.100.1:40975>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:56862 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:57230 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:56796 remote=tcp://172.30.100.1:40975>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53820 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53372 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53926 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53474 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53986 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53534 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53586 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54084 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53642 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53708 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54510 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55880 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54566 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55942 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55514 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:56000 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55572 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54684 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:56046 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55624 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54732 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55682 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54798 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55740 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54846 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55796 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54904 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54960 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55016 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55068 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55188 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55240 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55346 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55398 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55454 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53976 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53522 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53580 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53638 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:53694 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54556 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55932 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55502 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54612 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55557 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54674 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:56038 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55612 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54724 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55670 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54786 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55720 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55780 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54952 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55876 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55010 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55066 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55122 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55178 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55230 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55286 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55336 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55386 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55448 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55550 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54664 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55606 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54720 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54776 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54832 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54888 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54944 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55866 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55922 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55170 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55438 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:56020 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:56076 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55650 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54760 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55704 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54818 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55760 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54872 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55814 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:54932 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55910 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55100 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55154 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55210 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55272 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55316 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55370 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55424 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55478 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55592 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:57298 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:55166 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:56868 remote=tcp://172.30.100.1:40975>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.6.64:57304 remote=tcp://172.30.100.1:40975>
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:40975
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:34501
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:42616
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:35912
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:39139
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:40637
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:40174
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:40189
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:40322
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:42113
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:36686
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:33387
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:35618
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:46643
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:34760
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:41911
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:41123
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:43960
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:37268
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:42593
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:36786
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:36523
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:35917
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:39329
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:45622
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:34983
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:35414
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:42096
distributed.worker - INFO - Stopping worker at tcp://172.30.6.64:40835
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:42283'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:40047'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:34553'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:33079'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:41418'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:42116'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:38853'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:36391'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:33922'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:45976'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:42474'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:35558'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:46470'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:45382'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:39691'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:40076'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:40191'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:43271'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:38305'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:42865'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:37317'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:33554'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:33133'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:40685'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:39007'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:44916'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:39877'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.64:38480'
distributed.dask_worker - INFO - End worker
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
